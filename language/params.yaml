# Experiment tag to help with aggregating and filtering experiment sweeps
tag: quantized
# Global seed to make evaluation reproducible
seed: 2
# Model hyperparameters: ...
model:
  # Configuration of the quantized token embedding layer
  embedding:
    # Number of quantization bits for weights and activations of the embedding
    # layer, null means no quantization
    bits: 8
  # Configures the positional encoding layer in front of the first attention
  # block
  positional:
    # Type of positional encoding to use at the input (following the embedding)
    #   Options are: none, sinusoidal, binary, learned
    encoding: learned
    # Number of quantization bits for weights and activations of the encoding
    # layer, null means no quantization
    bits: 8
  # List of layers configuring the encoder stack: Either a string referring to a
  # pre-defined configuration or a list of individual layer configurations
  configuration: original
  # Number of layers, i.e., stacked repetitions of the encoder configuration
  # above
  num_layers: 2
  # Number of attention heads
  num_heads: 3
  # Embedding dimension shared by all blocks, i.e., embedding, attention, MLP,
  # convolution, etc.
  emb_dim: 192
  # Expanded embedding dimension in MLP or convolution blocks, usually this is
  # related to the embedding dimension as 4 * emb_dim
  expansion_dim: 768
  # Number of quantization bits for weights and activations (for all
  # intermediate layers), null means no quantization
  bits: 7
  # Number of quantization bits for weights and activations of the
  # output classification layer, null means no quantization
  cls_bits: 8
  # Activation functions to use after convolution and linear layers
  #   Options: tanh, sigmoid, relu, gelu, silu
  activation: relu
  # Normalization layer preceding or following each block
  #   Options: batch-norm, layer-norm, none
  norm: batch-norm
  # Placement of the normalization layer: pre-norm or post-norm, if norm is not
  # none
  norm_placement: pre-norm
  # Dropout: probability of an element to be zeroed during training
  dropout: 0.25
# Training/Validation dataset configuration
dataset:
  # Path to the dataset (huggingface identifier)
  path: roneneldan/TinyStories
  # Name of the dataset subset
  name:
  # Select the training split of the dataset
  split: train
  # Splits the dataset into train/validation/evaluation subsets
  splits:                       # 80%, 10%, 10% splits
# Tokenizer configuration
  - 0.80
  - 0.10
  - 0.10
tokenizer:
  # Pretrained base tokenizer used to train a custom tokenizer
  pretrained: bert-base-cased
  # Number of tokens in the vocabulary
  vocab_size: 2048
  # Limit the maximum different characters to keep in the alphabet
  limit_alphabet: 150
# Training hyperparameters
train:
  # Number of training epochs to run
  epochs: 50
  # Batch size for training
  batch_size: 256
  # Context length for training inputs
  context_length: 256
  # Whether to use masked language modeling
  mlm: true
  # Probability of random mask tokens
  mlm_probability: 0.15
  # Optimizer configuration
  optimizer:
    # Name of the optimization algorithm to use: adam or sgd
    algorithm: adam
    # (Initial) Learning rate
    lr: 0.001
    # L2 regularization
    weight_decay: 0.0
    # Coefficients used for computing running averages of gradient and its
    # square.
    betas:
    - 0.9
    - 0.999
    # Term added to the denominator to improve numerical stability
    eps: 1.0e-8
  # Loss function to use: cross-entropy or nll
  criterion: cross-entropy
  # Learning rate scheduler (ReduceLROnPlateau) configuration
  scheduler:
    # Reduce learning rate by  this factor if the loss stops improving
    factor: 0.5
    # Number of epochs without improvement to wait before reducing the
    # learning rate
    patience: 10
  # DataLoader keyword arguments
  loader:
    # Reshuffle data every epoch
    shuffle: true
    # Number of workers to use for loading the data in parallel
    num_workers: 32
    # Number of batches loaded in advance by each worker
    prefetch_factor: 4
    # Keep worker processes alive after consuming the dataset once
    persistent_workers: false
    # Load tensors into device pinned memory
    pin_memory: true
    # Drop the last batch if it is incomplete, i.e., smaller than the batch size
    drop_last: true
# Evaluation hyperparameters
eval:
  # Evaluation batch size - does not influence the result, just speeds up the
  # evaluation by doing more in parallel
  batch_size: 512
  # Context length for evaluation inputs
  context_length: 256
  # Whether to use masked language modeling
  mlm: true
  # Probability of random mask tokens
  mlm_probability: 0.15
  # DataLoader keyword arguments
  loader:
    # Number of workers to use for loading the data in parallel
    num_workers: 32
    # Number of batches loaded in advance by each worker
    prefetch_factor: 4
    # Keep worker processes alive after consuming the dataset once
    persistent_workers: false
    # Load tensors into device pinned memory
    pin_memory: true
    # Drop the last batch if it is incomplete
    drop_last: true
# Model to ONNX export hyperparameters
export:
  # Format to export: qcdq (~ standard ONNX) or qonnx
  format: qonnx
  # Explicitly splits all attention heads in the model graph to be parallel
  split_heads: true
  # Version of the default ONNX opset
  opset_version: 19
  # Apply the constant-folding optimization
  do_constant_folding: true
  # Export batch size - allows to provide more verification samples
  batch_size: 1
  # Context length for export inputs
  context_length: 256
  # Whether to use masked language modeling
  mlm: true
  # Probability of random mask tokens
  mlm_probability: 0.15
  # Need to use dynamo export to allow exporting torch.compiled graphs
  dynamo: false
  # Keep parameters as part of the model ONNX file
  external_data: false
  # Enables dynamo export model optimization
  optimize: false
