# Directory to store build output products, reports and logfiles
output_dir: build
# Output products to generate and keep: Reports, stitched-IP and deployment
generate_outputs:
  - estimate_reports
  - rtlsim_performance
  - stitched_ip
  - bitfile
  - pynq_driver
  - deployment_package
# Run synthesis to generate a .dcp for the stitched-IP output product
stitched_ip_gen_dcp: true
# Save the intermediate model graphs after each of the build steps, helpful for
# debugging and also allows to continue in the middle of the flow
save_intermediate_models: true

# Configure the build target: Device and shell, clock and target throughput
board: RFSoC2x2
shell_flow_type: vivado_zynq
synth_clk_period_ns: 10.0
target_fps: 1000

# Sequence of steps to transform, optimize and build the accelerator
steps:
  # Export step converting the already streamlined model (via onnx-passes) to
  # the FINN-compatible QONNX format
  # Note: When starting streamlining from scratch, uses step_passes_frontend
  - finn.builder.passes.export
  # Default FINN frontend step sequence: Streamlining
  - step_qonnx_to_finn
  - step_tidy_up
  - step_streamline
  # Customized adhoc hardware conversion step: Includes inferring the fused
  # operator for scaled dot-product attention
  - adhoc_steps.step_convert_to_hw
  # Default FINN partitioning and specialization steps
  - step_create_dataflow_partition
  - step_specialize_layers
  # Customized adhoc folding configuration: Tries to meet the throughput target
  # with each attention operator via semi-automatic folding and then hands over
  # to FINN auto-folding to configure all other operators before loading folding
  # configuration from a YAML file.
  - adhoc_steps.step_set_folding
  # Default FINN backend step sequence: Minimizing bit-width setting FIFOs, code
  # generation and synthesis
  - step_minimize_bit_width
  - step_generate_estimate_reports
#  - step_set_fifo_depths
  - step_hw_codegen
  - step_hw_ipgen
  - step_create_stitched_ip
  - step_measure_rtlsim_performance
  - step_out_of_context_synthesis
  - step_synthesize_bitfile
  - step_make_driver
  - step_deployment_package

# Select build steps at which the model should be executed on test inputs to
# verify correctness
verify_steps:
  - passes_frontend
  - finn_onnx_python
  - initial_python
  - streamlined_python
  - folded_hls_cppsim
  - node_by_node_rtlsim
  - stitched_ip_rtlsim
# Output full context dump for verification steps
verify_save_full_context: true
# Save .vcd waveforms from rtlsim
verify_save_rtlsim_waveforms: true
# File with test inputs for verification
verify_input_npy: inp.npy
# File with expected test outputs for verification
verify_expected_output_npy: out.npy
# Absolute tolerance for accepting verification outputs
verification_atol: 0.5

# Detailed per-operator or per-operator-type folding, attribute and backend
# specialization settings
folding_config_file: folding.yaml
specialize_layers_config_file: specialize_layers.json

# Force the implementation of standalone thresholds to be able to use RTL
# implementation of the MVU
standalone_thresholds: true
# Maximum bit-width of quantizers converted to multi-thresholds
# Note: Irrelevant for already converted and streamlined models
max_multithreshold_bit_width: 16
# Maximum width of MVAU stream per PE
mvau_wwidth_max: 2048
# FIFO nodes with depth larger than 32768 will be split
split_large_fifos: true

# Disable automatic FIFO-sizing: FIFO settings should be given in folding.yaml
auto_fifo_depths: false
