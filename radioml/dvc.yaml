# List experiment stages, i.e., the jobs to be run
stages:
  # Model training stage: Runs training of a classifier on the RadioML dataset
  train:
    # Run the stage relative to the project root directory
    wdir: ..
    # Stage runs the training script as the command
    cmd: bash run.sh python -m radioml.train
    # Data and code dependencies of this stage to determine when it needs to be
    # rerun
    deps:
      # RadioML case study source files
      - radioml/model.py
      - radioml/train.py
      - radioml/dataset.py
      # General source files shared by other experiments
      - activations.py
      - attention.py
      - blocks.py
      - embedding.py
      - encoding.py
      - lazy.py
      - quant.py
      - utils.py
    # Parameters from params.yaml used by this stage, changing any of these
    # triggers a rerun
    params:
      - radioml/params.yaml:
          - tag
          - seed
          - model
          - dataset
          - train
    # Outputs produced which should be tracked and passed on to the next stages
    outs:
      - outputs/radioml/model.pt
      - outputs/radioml/optimizer.pt
    # Plots produced by this stage
    plots:
      # Curves of training and validation loss per epoch
      - outputs/radioml/loss.yaml:
          # Explicitly plot the validation loss
          y: valid
          # Give a more readable title to the plot
          title: "RadioML Validation Loss per Epoch"
          # Track via git, not dvc cache
          cache: false
      # Curves of learning rate per epoch
      - outputs/radioml/lr.yaml:
          # Explicitly plot the learning rate at the start of the epoch
          y: last
          # Give a more readable title to the plot
          title: "RadioML Learning Rate per Epoch"
          # Track via git, not dvc cache
          cache: false
  train_INT8:
    # Run the stage relative to the project root directory
    wdir: ..
    # Stage runs the training script as the command
    cmd: INT8=1 bash run.sh python -m radioml.train
    # Data and code dependencies of this stage to determine when it needs to be
    # rerun
    deps:
      # RadioML case study source files
      - radioml/model.py
      - radioml/train.py
      - radioml/dataset.py
      # General source files shared by other experiments
      - activations.py
      - attention.py
      - blocks.py
      - embedding.py
      - encoding.py
      - lazy.py
      - quant.py
      - utils.py
    # Parameters from params.yaml used by this stage, changing any of these
    # triggers a rerun
    params:
      - radioml/params.yaml:
          - tag
          - seed
          - model_int8
          - dataset
          - train
    # Outputs produced which should be tracked and passed on to the next stages
    outs:
      - outputs/radioml/model_int8.pt
      - outputs/radioml/optimizer_int8.pt
  eval:
    # Run the stage relative to the project root directory
    wdir: ..
    # Stage runs the evaluation script as the command
    cmd: bash run.sh python -m radioml.eval
    # Data and code dependencies of this stage to determine when it needs to be
    # rerun
    deps:
      # RadioML case study source files
      - radioml/model.py
      - radioml/eval.py
      - radioml/dataset.py
      # General source files shared by other experiments
      - activations.py
      - attention.py
      - blocks.py
      - embedding.py
      - encoding.py
      - lazy.py
      - quant.py
      - utils.py
      # The model checkpoint produced by the training stage
      - outputs/radioml/model.pt
    # Parameters from params.yaml used by this stage, changing any of these
    # triggers a rerun
    params:
      - radioml/params.yaml:
          - tag
          - seed
          - model
          - dataset
          - eval
    # Metrics produced by this stage
    metrics:
      # Classification accuracy over the evaluation dataset
      - outputs/radioml/accuracy.yaml:
          # Track via git, not dvc cache
          cache: false
    # Plots produced by this stage
    plots:
      # Confusion matrix of predicted vs. true classes
      - outputs/radioml/classes.csv:
          # Use true class label as x-axis
          x: cls
          # Use the predicted class label as y-axis
          y: prediction
          # Use the confusion matrix plot template
          template: confusion
          # Give a more readable title to the plot
          title: "RadioML Confusion Matrix"
          # Track via git, not dvc cache
          cache: false
  # Model export stage: Exports the trained classifier to ONNX format alongside
  # some verification samples from the RadioML dataset
  export:
    # Run the stage relative to the project root directory
    wdir: ..
    # Stage runs the export script as the command
    cmd: python -m radioml.export
    # Data and code dependencies of this stage to determine when it needs to be
    # rerun
    deps:
      # RadioML case study source files
      - radioml/model.py
      - radioml/export.py
      - radioml/dataset.py
      # General source files shared by other experiments
      - activations.py
      - attention.py
      - blocks.py
      - embedding.py
      - encoding.py
      - lazy.py
      - quant.py
      - utils.py
      # The model checkpoint produced by the training stage
      - outputs/radioml/model.pt
    # Parameters from params.yaml used by this stage, changing any of these
    # triggers a rerun
    params:
      - radioml/params.yaml:
          - tag
          - seed
          - model
          - model_int8
          - dataset
          - export
          - batch_sizes
    # Outputs produced which should be tracked and passed on to the next stages
    outs:
      # Trained model exported to the ONNX format
      - outputs/radioml/model.onnx
      # Model verification input/output samples from the RadioML dataset
      - outputs/radioml/inp.npy
      - outputs/radioml/out.npy
      - outputs/radioml/model_dynamic_batchsize.onnx
  # TODO: FINN-build and deployment stages...
  measure_32FP:
    wdir: ..
    cmd: FP16=0 python radioml/measure.py
    deps:
      - outputs/radioml/model_dynamic_batchsize.onnx
      - radioml/measure.py
      - radioml/export.py
    params:
      - radioml/params.yaml:
          - batch_sizes
    plots:
     -  outputs/radioml/throughput/FP32/throughput_results.json:
         template: plot_templates/bar_2.json
         x: batch_size
         y: throughput_batches_per_s
         cache: false
     - outputs/radioml/throughput/FP32/throughput_results_2.json:
         template: plot_templates/bar_1.json
         x: batch_size
         y: throughput_images_per_s
         cache: false
     - outputs/radioml/throughput/FP32/latency_results.json:
         template: plot_templates/bar_3.json
         x: batch_size
         y: value
         cache: false
     - outputs/radioml/throughput/FP32/latency_results_batch.json:
         template: plot_templates/bar_4.json
         x: batch_size
         y: value
         cache: false
  measure_16FP:
    wdir: ..
    cmd: FP16=1 python radioml/measure.py
    deps:
      - outputs/radioml/model_dynamic_batchsize.onnx
      - radioml/measure.py
      - radioml/export.py
    params:
      - radioml/params.yaml:
          - batch_sizes
    plots:
     -  outputs/radioml/throughput/FP16/throughput_results.json:
         template: plot_templates/bar_2_FP16.json
         x: batch_size
         y: throughput_batches_per_s
         cache: false
     - outputs/radioml/throughput/FP16/throughput_results_2.json:
         template: plot_templates/bar_1_FP16.json
         x: batch_size
         y: throughput_images_per_s
         cache: false
     - outputs/radioml/throughput/FP16/latency_results.json:
         template: plot_templates/bar_3_FP16.json
         x: batch_size
         y: value
         cache: false
     - outputs/radioml/throughput/FP16/latency_results_batch.json:
         template: plot_templates/bar_4_FP16.json
         x: batch_size
         y: value
         cache: false
  measure_INT8_brevitas:
    wdir: ..
    cmd: INT8=1 python radioml/measure.py
    deps:
      - radioml/measure.py
      - plot_templates/bar_1_INT8.json
      - plot_templates/bar_2_INT8.json
      - plot_templates/bar_3_INT8.json
      - plot_templates/bar_4_INT8.json
    params:
      - radioml/params.yaml:
          - batch_sizes
    outs:
      - outputs/radioml/eval_results/accuracy_INT8.json
    plots:
     -  outputs/radioml/throughput/INT8/throughput_results.json:
         template: plot_templates/bar_2_INT8.json
         x: batch_size
         y: throughput_batches_per_s
         cache: false
     - outputs/radioml/throughput/INT8/throughput_results_2.json:
         template: plot_templates/bar_1_INT8.json
         x: batch_size
         y: throughput_images_per_s
         cache: false
     - outputs/radioml/throughput/INT8/latency_results.json:
         template: plot_templates/bar_3_INT8.json
         x: batch_size
         y: value
         cache: false
     - outputs/radioml/throughput/INT8/latency_results_batch.json:
         template: plot_templates/bar_4_INT8.json
         x: batch_size
         y: value
         cache: false
  quantize_tensorrt_INT8:
    wdir: ..
    cmd: python radioml/quantize_tensorrt.py 
    deps:
      - radioml/quantize_tensorrt.py
      - outputs/radioml/model_dynamic_batchsize.onnx
    params:
      - radioml/params.yaml:
          - batch_sizes
    outs:
      - outputs/radioml/engines
  measure_INT8_tensorrt:
    wdir: ..
    cmd: python radioml/measure_8bit.py
    deps:
      - radioml/measure_8bit.py
      - plot_templates/bar_1_INT8_tensorrt.json
      - plot_templates/bar_2_INT8_tensorrt.json
      - plot_templates/bar_3_INT8_tensorrt.json
      - plot_templates/bar_4_INT8_tensorrt.json
      - outputs/radioml/engines
    params:
      - radioml/params.yaml:
          - batch_sizes
    outs:
      - outputs/radioml/eval_results/accuracy_INT8_tensorrt.json
    plots:
     -  outputs/radioml/throughput/INT8_tensorrt/throughput_results.json:
         template: plot_templates/bar_2_INT8_tensorrt.json
         x: batch_size
         y: throughput_batches_per_s
         cache: false
     - outputs/radioml/throughput/INT8_tensorrt/throughput_results_2.json:
         template: plot_templates/bar_1_INT8_tensorrt.json
         x: batch_size
         y: throughput_images_per_s
         cache: false
     - outputs/radioml/throughput/INT8_tensorrt/latency_results.json:
         x: batch_size
         y: value
         cache: false
     - outputs/radioml/throughput/INT8_tensorrt/latency_results_batch.json:
         template: plot_templates/bar_4_INT8_tensorrt.json
         x: batch_size
         y: value
         cache: false
  accuracy_comparison:
    wdir: ..
    cmd: python radioml/accuracy_comparison.py
    deps:
      - radioml/accuracy_comparison.py
      - outputs/radioml/eval_results/accuracy_FP32.json
      - outputs/radioml/eval_results/accuracy_FP16.json
      - outputs/radioml/eval_results/accuracy_INT8.json
      - outputs/radioml/eval_results/accuracy_INT8_tensorrt.json
    params:
      - radioml/params.yaml:
          - batch_sizes
    plots:
     -  outputs/radioml/eval_results/accuracy.json:
         template: plot_templates/bar_accuracy.json
         x: quantisation_type
         y: value
         cache: false
  energy_measure_FP32:
    wdir: ..
    cmd: python radioml/tegrastats_new.py
    deps:
      - radioml/tegrastats_new.py
      - radioml/parse_tegrastats_to_json.py
    params:
      - radioml/params.yaml:
          - batch_sizes
    outs:
      - outputs/radioml/energy_metrics/energy_metrics.json
      - outputs/radioml/energy_metrics/ram_metrics_2.json
      - outputs/radioml/energy_metrics/energy_consumption_2.json
    plots:
     -  outputs/radioml/energy_metrics/ram_metrics.json:
          template: plot_templates/ram_used.json
          x: timestamp
          y: ram_used
          cache: false
     -  outputs/radioml/energy_metrics/energy_consumption.json:
          template: plot_templates/energy_consumption.json
          x: timestamp
          y: value
          cache: false


         # add measure (without tensorrt int 8 bc. it just works with nonquant layers) and measure energy for radioml
         # problem: bits=8 für brevitas nötig, "RuntimeError: CUDA error: no kernel image is available for execution on the device" nachdem train nochmal laufen gelassen wurde (evtl. wegen neuen installationen)
         # Christoph fragen - Version von torch muss für measure 2.6 sein, aber für train 2.8?


         # welche Bausteine sind gemeint? BERT anpassen
         # keine quantize/dequantize layer mehr - die hatte ich in meinem modell sowieso nicht drin
         # stattdessen lazyquantlinear und lazydequantlinear nutzen? aber mt dem bert mache ich das ja eh anders mit pretrained model und mit diesem Beispielcode von brevitas, bits parameter nutzen?
         # benchmark modell anschauen - soll da auch measure laufen?
 



         # https://github.com/iksnagreb/onnx-passes anschauen
         # ausprobieren: ob du mit dem "import-qonnx" (siehe demo/quant) ein von Brevitas als QONNX exportiertes Modell mit TensorRT immerhin ausführen kannst


         
         # vergleichen: TensorRT so wie du es bisher machst und TensorRT als Execution Provider

