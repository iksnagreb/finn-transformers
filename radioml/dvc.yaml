# List experiment stages, i.e., the jobs to be run
stages:
  # Model training stage: Runs training of a classifier on the RadioML dataset
  train:
    # Run the stage relative to the project root directory
    wdir: ..
    # Stage runs the training script as the command
    cmd: bash run.sh python -m radioml.train
    # Data and code dependencies of this stage to determine when it needs to be
    # rerun
    deps:
      # RadioML case study source files
      - radioml/model.py
      - radioml/train.py
      - radioml/dataset.py
      # General source files shared by other experiments
      - activations.py
      - attention.py
      - blocks.py
      - embedding.py
      - encoding.py
      - lazy.py
      - quant.py
      - utils.py
    # Parameters from params.yaml used by this stage, changing any of these
    # triggers a rerun
    params:
      - radioml/params.yaml:
          - tag
          - seed
          - model
          - dataset
          - train
    # Outputs produced which should be tracked and passed on to the next stages
    outs:
      - outputs/radioml/model.pt
      - outputs/radioml/optimizer.pt
    # Plots produced by this stage
    plots:
      # Curves of training and validation loss per epoch
      - outputs/radioml/loss.yaml:
          # Explicitly plot the validation loss
          y: valid
          # Give a more readable title to the plot
          title: "RadioML Validation Loss per Epoch"
          # Track via git, not dvc cache
          cache: false
      # Curves of learning rate per epoch
      - outputs/radioml/lr.yaml:
          # Explicitly plot the learning rate at the start of the epoch
          y: last
          # Give a more readable title to the plot
          title: "RadioML Learning Rate per Epoch"
          # Track via git, not dvc cache
          cache: false
  # Model evaluation stage: Evaluates the trained model on the RadioML dataset
  eval:
    # Run the stage relative to the project root directory
    wdir: ..
    # Stage runs the evaluation script as the command
    cmd: bash run.sh python -m radioml.eval
    # Data and code dependencies of this stage to determine when it needs to be
    # rerun
    deps:
      # RadioML case study source files
      - radioml/model.py
      - radioml/eval.py
      - radioml/dataset.py
      # General source files shared by other experiments
      - activations.py
      - attention.py
      - blocks.py
      - embedding.py
      - encoding.py
      - lazy.py
      - quant.py
      - utils.py
      # The model checkpoint produced by the training stage
      - outputs/radioml/model.pt
    # Parameters from params.yaml used by this stage, changing any of these
    # triggers a rerun
    params:
      - radioml/params.yaml:
          - tag
          - seed
          - model
          - dataset
          - eval
    # Metrics produced by this stage
    metrics:
      # Classification accuracy over the evaluation dataset
      - outputs/radioml/accuracy.yaml:
          # Track via git, not dvc cache
          cache: false
    # Plots produced by this stage
    plots:
      # Confusion matrix of predicted vs. true classes
      - outputs/radioml/classes.csv:
          # Use true class label as x-axis
          x: cls
          # Use the predicted class label as y-axis
          y: prediction
          # Use the confusion matrix plot template
          template: confusion
          # Give a more readable title to the plot
          title: "RadioML Confusion Matrix"
          # Track via git, not dvc cache
          cache: false
  # Model export stage: Exports the trained classifier to ONNX format alongside
  # some verification samples from the RadioML dataset
  export:
    # Run the stage relative to the project root directory
    wdir: ..
    # Stage runs the export script as the command
    cmd: python -m radioml.export
    # Data and code dependencies of this stage to determine when it needs to be
    # rerun
    deps:
      # RadioML case study source files
      - radioml/model.py
      - radioml/export.py
      - radioml/dataset.py
      # General source files shared by other experiments
      - activations.py
      - attention.py
      - blocks.py
      - embedding.py
      - encoding.py
      - lazy.py
      - quant.py
      - utils.py
      # The model checkpoint produced by the training stage
      - outputs/radioml/model.pt
    # Parameters from params.yaml used by this stage, changing any of these
    # triggers a rerun
    params:
      - radioml/params.yaml:
          - tag
          - seed
          - model
          - dataset
          - export
    # Outputs produced which should be tracked and passed on to the next stages
    outs:
      # Trained model exported to the ONNX format
      - outputs/radioml/model.onnx
      # Model verification input/output samples from the RadioML dataset
      - outputs/radioml/inp.npy
      - outputs/radioml/out.npy
      - outputs/radioml/model_dynamic_batchsize.onnx
  # TODO: FINN-build and deployment stages...
  measure_32FP:
    wdir: ..
    cmd: FP16=0 python radioml/measure.py
    deps:
      - outputs/radioml/model_dynamic_batchsize.onnx
      - radioml/measure.py
      - radioml/export.py
    plots:
     -  outputs/radioml/throughput/FP32/throughput_results.json:
         template: plot_templates/bar_2.json
         x: batch_size
         y: throughput_batches_per_s
         cache: false
     - outputs/radioml/throughput/FP32/throughput_results_2.json:
         template: plot_templates/bar_1.json
         x: batch_size
         y: throughput_images_per_s
         cache: false
     - outputs/radioml/throughput/FP32/latency_results.json:
         template: plot_templates/bar_3.json
         x: batch_size
         y: value
         cache: false
     - outputs/radioml/throughput/FP32/latency_results_batch.json:
         template: plot_templates/bar_4.json
         x: batch_size
         y: value
         cache: false
  measure_16FP:
    wdir: ..
    cmd: FP16=1 python radioml/measure.py
    deps:
      - outputs/radioml/model_dynamic_batchsize.onnx
      - radioml/measure.py
      - radioml/export.py
    plots:
     -  outputs/radioml/throughput/FP16/throughput_results.json:
         template: plot_templates/bar_2_FP16.json
         x: batch_size
         y: throughput_batches_per_s
         cache: false
     - outputs/radioml/throughput/FP16/throughput_results_2.json:
         template: plot_templates/bar_1_FP16.json
         x: batch_size
         y: throughput_images_per_s
         cache: false
     - outputs/radioml/throughput/FP16/latency_results.json:
         template: plot_templates/bar_3_FP16.json
         x: batch_size
         y: value
         cache: false
     - outputs/radioml/throughput/FP16/latency_results_batch.json:
         template: plot_templates/bar_4_FP16.json
         x: batch_size
         y: value
         cache: false
  measure_INT8_brevitas:
    wdir: ..
    cmd: INT8=1 python radioml/measure.py
    deps:
      - radioml/measure.py
      - plot_templates/bar_1_INT8.json
      - plot_templates/bar_2_INT8.json
      - plot_templates/bar_3_INT8.json
      - plot_templates/bar_4_INT8.json
    outs:
      - outputs/radioml/eval_results/accuracy_INT8.json
    plots:
     -  outputs/radioml/throughput/INT8/throughput_results.json:
         template: plot_templates/bar_2_INT8.json
         x: batch_size
         y: throughput_batches_per_s
         cache: false
     - outputs/radioml/throughput/INT8/throughput_results_2.json:
         template: plot_templates/bar_1_INT8.json
         x: batch_size
         y: throughput_images_per_s
         cache: false
     - outputs/radioml/throughput/INT8/latency_results.json:
         template: plot_templates/bar_3_INT8.json
         x: batch_size
         y: value
         cache: false
     - outputs/radioml/throughput/INT8/latency_results_batch.json:
         template: plot_templates/bar_4_INT8.json
         x: batch_size
         y: value
         cache: false
  measure_INT8_tensorrt:
    cmd: python radioml/measure_8bit.py
    deps:
      - radioml/measure_8bit.py
      - plot_templates/bar_1_INT8_tensorrt.json
      - plot_templates/bar_2_INT8_tensorrt.json
      - plot_templates/bar_3_INT8_tensorrt.json
      - plot_templates/bar_4_INT8_tensorrt.json
      - outputs/radioml/engines
    outs:
      - eval_results/accuracy_INT8_tensorrt.json
    plots:
     -  throughput/INT8_tensorrt/throughput_results.json:
         template: plot_templates/bar_2_INT8_tensorrt.json
         x: batch_size
         y: throughput_batches_per_s
         cache: false
     - throughput/INT8_tensorrt/throughput_results_2.json:
         template: plot_templates/bar_1_INT8_tensorrt.json
         x: batch_size
         y: throughput_images_per_s
         cache: false
     - throughput/INT8_tensorrt/latency_results.json:
         template: plot_templates/bar_3_INT8_tensorrt.json
         x: batch_size
         y: value
         cache: false
     - throughput/INT8_tensorrt/latency_results_batch.json:
         template: plot_templates/bar_4_INT8_tensorrt.json
         x: batch_size
         y: value
         cache: false


         # add measure (without tensorrt int 8 bc. it just works with nonquant layers) and measure energy for radioml
         # problem: bits=8 für brevitas nötig, RuntimeError: CUDA error: no kernel image is available for execution on the device nachdem train nochmal laufen gelassen wurde (ev. wegen neuen installationen)
         # welche Bausteine sind gemeint? BERT anpassen
         # https://github.com/iksnagreb/onnx-passes anschauen
         # ausprobieren: ob du mit dem "import-qonnx" (siehe demo/quant) ein von Brevitas als QONNX exportiertes Modell mit TensorRT immerhin ausführen kannst
         # vergleichen: TensorRT so wie du es bisher machst und TensorRT als Execution Provider

