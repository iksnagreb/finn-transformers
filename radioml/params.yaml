# Experiment tag to help with aggregating and filtering experiment sweeps
tag: baseline
# Global seed to make evaluation reproducible
seed: 12
# Model hyperparameters: ...
batch_sizes: [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]
model:
  # Number of output classes to predict (default 24 RadioML classes)
  num_classes: 24
  # Configuration of the initial patch-embedding layer
  embedding:
    # Number of patches to aggregate the feature map into
    patches: [ 1, 64 ]
    # Kernel size for sliding windows generating the patches
    kernel_size: [ 1, 16 ]
    # Stride of the sliding windows
    stride: [ 1, 16 ]
    # Activation functions to use after the convolution layer
    #   Options: tanh, sigmoid, relu, gelu, silu, none
    activation: relu
    # Number of quantization bits for weights and activations of the embedding
    # layer, null means no quantization
    bits: null
  # Type of positional encoding to use at the input (following the embedding)
  #   Options are: none, sinusoidal, binary, learned
  positional: binary
  # List of layers configuring the encoder stack: Either a string referring to a
  # pre-defined configuration or a list of individual layer configurations
  configuration: original
  # Number of layers, i.e., stacked repetitions of the encoder configuration
  # above
  num_layers: 1
  # Number of attention heads
  num_heads: 4
  # Embedding dimension shared by all blocks, i.e., embedding, attention, MLP,
  # convolution, etc.
  emb_dim: 32
  # Expanded embedding dimension in MLP or convolution blocks, usually this is
  # related to the embedding dimension as 4 * emb_dim
  expansion_dim: 128
  # Number of quantization bits for weights and activations (for all
  # intermediate layers), null means no quantization
  bits: null
  # Activation functions to use after convolution and linear layers
  #   Options: tanh, sigmoid, relu, gelu, silu
  activation: relu
  # Normalization layer preceding or following each block
  #   Options: batch-norm, layer-norm, none
  norm: batch-norm
  # Placement of the normalization layer: pre-norm or post-norm, if norm is not
  # none
  norm_placement: post-norm
  # Dropout: probability of an element to be zeroed during training
  dropout: 0.0
# Training/Validation dataset configuration
model_int8:
  # Number of output classes to predict (default 24 RadioML classes)
  num_classes: 24
  # Configuration of the initial patch-embedding layer
  embedding:
    # Number of patches to aggregate the feature map into
    patches: [ 1, 64 ]
    # Kernel size for sliding windows generating the patches
    kernel_size: [ 1, 16 ]
    # Stride of the sliding windows
    stride: [ 1, 16 ]
    # Activation functions to use after the convolution layer
    #   Options: tanh, sigmoid, relu, gelu, silu, none
    activation: relu
    # Number of quantization bits for weights and activations of the embedding
    # layer, null means no quantization
    bits: 8
  # Type of positional encoding to use at the input (following the embedding)
  #   Options are: none, sinusoidal, binary, learned
  positional: binary
  # List of layers configuring the encoder stack: Either a string referring to a
  # pre-defined configuration or a list of individual layer configurations
  configuration: original
  # Number of layers, i.e., stacked repetitions of the encoder configuration
  # above
  num_layers: 1
  # Number of attention heads
  num_heads: 4
  # Embedding dimension shared by all blocks, i.e., embedding, attention, MLP,
  # convolution, etc.
  emb_dim: 32
  # Expanded embedding dimension in MLP or convolution blocks, usually this is
  # related to the embedding dimension as 4 * emb_dim
  expansion_dim: 128
  # Number of quantization bits for weights and activations (for all
  # intermediate layers), null means no quantization
  bits: 8
  # Activation functions to use after convolution and linear layers
  #   Options: tanh, sigmoid, relu, gelu, silu
  activation: relu
  # Normalization layer preceding or following each block
  #   Options: batch-norm, layer-norm, none
  norm: batch-norm
  # Placement of the normalization layer: pre-norm or post-norm, if norm is not
  # none
  norm_placement: post-norm
  # Dropout: probability of an element to be zeroed during training
  dropout: 0.0
# Training/Validation dataset configuration
dataset:
  # Optionally select only a subset of the classes
  #  classes: null
  # Optionally select only a subset of the available noise levels
  signal_to_noise_ratios: [
    # Noise levels from -6 dB upwards to 30 dB in steps of 2
    #   !!python/object/apply:builtins.range [-6, 31, 2]
    -6, -4, -2, 0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30
  ]
  # Splits the dataset into train/validation/evaluation subsets
  splits: [ 0.70, 0.15, 0.15 ]  # 70%, 15%, 15% splits
  # Seed used for reproducible splitting of datasets
  seed: 12
  # Reshape the dataset to fit the expected embedding dimension of the model
  # and to effectively reduce the sequence length
  reshape: [ 1, 1024, 2 ]
# Training hyperparameters
train:
  # Number of training epochs to run
  epochs: 1
  # Batch size for training
  batch_size: 512
  # Optimizer configuration
  optimizer:
    # Name of the optimization algorithm to use: adam or sgd
    algorithm: adam
    # (Initial) Learning rate
    lr: 0.001
    # L2 regularization
    weight_decay: 0.0
    # Coefficients used for computing running averages of gradient and its
    # square.
    betas: [ 0.9, 0.98 ]
    # Term added to the denominator to improve numerical stability
    eps: 1.0e-8
  # Loss function to use: cross-entropy or nll
  criterion: cross-entropy
  # Learning rate scheduler (ReduceLROnPlateau) configuration
  scheduler:
    # Reduce learning rate by  this factor if the loss stops improving
    factor: 0.1
    # Number of epochs without improvement to wait before reducing the
    # learning rate
    patience: 10
  # DataLoader keyword arguments
  loader:
    # Reshuffle data every epoch
    shuffle: true
    # Number of workers to use for loading the data in parallel
    num_workers: 32
    # Number of batches loaded in advance by each worker
    prefetch_factor: 4
    # Keep worker processes alive after consuming the dataset once
    persistent_workers: false
    # Load tensors into device pinned memory
    pin_memory: true
    # Drop the last batch if it is incomplete, i.e., smaller than the batch size
    drop_last: true
# Evaluation hyperparameters
eval:
  # Evaluation batch size - does not influence the result, just speeds up the
  # evaluation by doing more in parallel
  batch_size: 512
  # DataLoader keyword arguments
  loader:
    # Number of workers to use for loading the data in parallel
    num_workers: 8
    # Number of batches loaded in advance by each worker
    prefetch_factor: 8
    # Load tensors into device pinned memory
    pin_memory: true
    # Do not drop the last batch if it is incomplete
    drop_last: false
# Model to ONNX export hyperparameters
export:
  # Explicitly splits all attention heads in the model graph to be parallel
  split_heads: false
  # Version of the default ONNX opset
  opset_version: 18
  # Apply the constant-folding optimization
  do_constant_folding: true
  # Export batch size - allows to provide more verification samples
  batch_size: 1
  # Need to use dynamo export to allow exporting torch.compiled graphs
  dynamo: false
  # Keep parameters as part of the model ONNX file
  external_data: false
  # Enables dynamo export model optimization
  optimize: false
