# Multihead Self Attention block according to Vaswani et al. 2017.
from blocks import Attention as Model  # noqa: Unused, just alias...
