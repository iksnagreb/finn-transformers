# Experiment tag to help with aggregating and filtering experiment sweeps
tag: baseline
# Global seed to make evaluation reproducible
seed: 12
# Model hyperparameters: ...
model:
  # Number of output classes to predict (default 10 MNIST/CIFAR-10 classes)
  num_classes: 10  # TODO: Derive from dataset...?
  # Configuration of the initial patch-embedding layer
  embedding:
    # Number of patches to aggregate the feature map into
    # "An Image is worth 16x16 words"...
    patches:
    - 16
    - 16
    # Kernel size for sliding windows generating the patches
    kernel_size:
    - 2
    - 2
    # Padding to the input width to ensure expected number of output patches
    padding:
    - 0
    - 0
    # Stride of the sliding windows
    stride:
    - 2
    - 2
    # Activation functions to use after the convolution layer
    #   Options: tanh, sigmoid, relu, gelu, silu, none
    activation: relu
    # Number of quantization bits for weights and activations of the embedding
    # layer, null means no quantization
    bits:
  # Type of positional encoding to use at the input (following the embedding)
  #   Options are: none, sinusoidal, binary, learned
  positional: learned
  # List of layers configuring the encoder stack: Either a string referring to a
  # pre-defined configuration or a list of individual layer configurations
  configuration: original
  # Number of layers, i.e., stacked repetitions of the encoder configuration
  # above
  num_layers: 4
  # Number of attention heads
  num_heads: 1
  # Embedding dimension shared by all blocks, i.e., embedding, attention, MLP,
  # convolution, etc.
  emb_dim: 16
  # Expanded embedding dimension in MLP or convolution blocks, usually this is
  # related to the embedding dimension as 4 * emb_dim
  expansion_dim: 32
  # Number of quantization bits for weights and activations (for all
  # intermediate layers), null means no quantization
  bits:
  # Activation functions to use after convolution and linear layers
  #   Options: tanh, sigmoid, relu, gelu, silu
  activation: relu
  # Normalization layer preceding or following each block
  #   Options: batch-norm, layer-norm, none
  norm: batch-norm
  # Placement of the normalization layer: pre-norm or post-norm, if norm is not
  # none
  norm_placement: post-norm
  # Dropout: probability of an element to be zeroed during training
  dropout: 0.1
# Training/Validation dataset configuration
dataset: '...'
# Training hyperparameters
train:
  # Number of training epochs to run
  epochs: 30
  # Batch size for training
  batch_size: 256
  # Optimizer configuration
  optimizer:
    # Name of the optimization algorithm to use: adam or sgd
    algorithm: adam
    # (Initial) Learning rate
    lr: 0.01
    # L2 regularization
    weight_decay: 1.0e-5
    # Coefficients used for computing running averages of gradient and its
    # square.
    betas:
    - 0.9
    - 0.999
    # Term added to the denominator to improve numerical stability
    eps: 1.0e-8
  # Loss function to use: cross-entropy or nll
  criterion: cross-entropy
  # Learning rate scheduler (ReduceLROnPlateau) configuration
  scheduler:
    # Reduce learning rate by  this factor if the loss stops improving
    factor: 0.5
    # Number of epochs without improvement to wait before reducing the
    # learning rate
    patience: 10
  # DataLoader keyword arguments
  loader:
    # Reshuffle data every epoch
    shuffle: true
    # Number of workers to use for loading the data in parallel
    num_workers: 32
    # Number of batches loaded in advance by each worker
    prefetch_factor: 4
    # Keep worker processes alive after consuming the dataset once
    persistent_workers: false
    # Load tensors into device pinned memory
    pin_memory: true
    # Drop the last batch if it is incomplete, i.e., smaller than the batch size
    drop_last: true
# Evaluation hyperparameters
eval:
  # Evaluation batch size - does not influence the result, just speeds up the
  # evaluation by doing more in parallel
  batch_size: 512
  # DataLoader keyword arguments
  loader:
    # Number of workers to use for loading the data in parallel
    num_workers: 32
    # Number of batches loaded in advance by each worker
    prefetch_factor: 4
    # Keep worker processes alive after consuming the dataset once
    persistent_workers: false
    # Load tensors into device pinned memory
    pin_memory: true
    # Do not drop the last batch if it is incomplete
    drop_last: false
# Model to ONNX export hyperparameters
export:
  # Explicitly splits all attention heads in the model graph to be parallel
  split_heads: false
  # Version of the default ONNX opset
  opset_version: 19
  # Apply the constant-folding optimization
  do_constant_folding: true
  # Export batch size - allows to provide more verification samples
  batch_size: 1
  # Need to use dynamo export to allow exporting torch.compiled graphs
  dynamo: false
  # Keep parameters as part of the model ONNX file
  external_data: false
  # Enables dynamo export model optimization
  optimize: false
